---
title: Neural Network ƒë∆°n gi·∫£n v·ªõi C++ (Ph·∫ßn 1)
create_date: 07/15/2024
update_date: 07/24/2024
tags:
    - deep learning
    - study
draft: true
thumbnail: https://i.redd.it/zjxo20plpms81.jpg
---

> To√†n b·ªô source code n·∫±m ·ªü: https://github.com/lenguyen1807/mnist-cpp

## Neural Network l√† g√¨ ?

<MdxImage
src="https://images.prismic.io/turing/659d7aad531ac2845a274355_Deep_neural_network_25a8b6fa7f.webp?auto=format,compress"
alt="Neural Network Architecture"
caption="H√¨nh 1: Ki·∫øn tr√∫c c·ªßa m·ªôt Neural Network (ngu·ªìn: Understanding Feed Forward Neural Networks With Maths and Statistics - turing.com)"
/>

D·ª±a v√†o ·∫£nh tr√™n, c√≥ th·ªÉ th·∫•y m·ªôt Neural Network nh∆∞ l√† m·ªôt ƒë·ªì th·ªã c√≥ h∆∞·ªõng kh√¥ng chu tr√¨nh. Trong ƒë√≥ s·∫Ω g·ªìm nhi·ªÅu n√∫t, m·ªói n√∫t ƒë∆∞·ª£c g·ªçi l√† m·ªôt **Neuron** v√† ƒë∆∞·ª£c s·∫Øp x·∫øp theo t·ª´ng **layer**, nh·ªØng layer m√† kh√¥ng ph·∫£i input ho·∫∑c output th√¨ ƒë∆∞·ª£c g·ªçi l√† **hidden layer**. M·ªói c·∫°nh c·ªßa Neural Network s·∫Ω c√≥ tr·ªçng s·ªë hay **weight**.

> L∆∞u √Ω l√† khi ta n√≥i Neural Network c√≥ 1 layer, ta s·∫Ω hi·ªÉu Neural Network ·∫•y s·∫Ω bao g·ªìm 1 layer input v√† 1 layer output. B·ªüi v√¨ khi n√≥i ƒë·∫øn layer, ta ch·ªâ n√≥i ƒë·∫øn c√°c layer c√≥ th·ªÉ t√≠nh to√°n ƒë∆∞·ª£c, c√≤n input layer ch·ªâ c√≥ nhi·ªám v·ª• nh·∫≠n gi√° tr·ªã ch·ª© kh√¥ng t√≠nh to√°n gi√° tr·ªã ·∫•y <MdxCite bibKey="aggarwalIntroductionNeuralNetworks2023"/>.

Neural Network ch·ªâ c√≥ 1 layer (g·ªìm input layer v√† output layer) ƒë∆∞·ª£c g·ªçi l√† **Perceptron**. Tr∆∞·ªõc khi ƒëi v√†o Neural Network nhi·ªÅu layer h∆°n, ta s·∫Ω t√¨m hi·ªÉu m·ªôt ch√∫t v·ªÅ Perceptron.

> M·ªôt ph·∫ßn l∆∞u √Ω n·ªØa, Neural Network c√≥ th·ªÉ l√† m·ªôt ƒë·ªì th·ªã c√≥ chu tr√¨nh v√† s·∫Ω ƒë∆∞·ª£c g·ªçi l√† **Recurrent Neural Network**, c√≤n Neural Network m√† ch√∫ng ta ƒëang n√≥i t·ªõi (c√≥ h∆∞·ªõng kh√¥ng chu tr√¨nh) ƒë∆∞·ª£c g·ªçi l√† **Feed Forward Neural Network**. V√† m·ªói khi nh·∫Øc ƒë·∫øn Neural Network trong b√†i n√†y ta s·∫Ω hi·ªÉu n√≥ l√† Feed Forward Neural Network nh√©. Ngo√†i ra m·ªói neuron ƒë·ªÅu c√≥ c·∫°nh n·ªëi t·ªõi n√≥, do ƒë√≥ c√≥ th·ªÉ nghe ƒë·∫øn term **Fully-Connected Neural Network**.

### Perceptron

<MdxImage
src="https://drive.google.com/uc?export=view&id=1H8WUAQmKet219-O4uNAeY8BUgdOwWjHj"
alt="Perceptron Image"
caption="H√¨nh 2: M·ªôt v√≠ d·ª• v·ªÅ perceptron"
/>

> √ù t∆∞·ªüng m·ªü r·ªông output neuron ƒë∆∞·ª£c m√¨nh l·∫•y th·∫≥ng t·ª´ <MdxCite bibKey="aggarwalIntroductionNeuralNetworks2023"/>.

·ªû h√¨nh 2 l√† v√≠ d·ª• v·ªÅ m·ªôt perceptron g·ªìm $N$ input v√† 1 output do ƒë√≥ input layer s·∫Ω g·ªìm $N$ neuron v√† output layer ch·ªâ c√≥ 1 neuron. C√≥ th·ªÉ th·∫•y, neuron output $y$ s·∫Ω ƒë∆∞·ª£c t√≠nh b·∫±ng t·ªï h·ª£p tuy·∫øn t√≠nh c·ªßa c√°c input neuron (trong ƒë√≥ c√≥ c√°c tr·ªçng s·ªë $w_i$ t∆∞∆°ng ·ª©ng v·ªõi input neuron $x_i$), sau ƒë√≥ t·ªï h·ª£p tuy·∫øn t√≠nh ·∫•y s·∫Ω ƒëi qua m·ªôt h√†m $\phi$.
$$
y = \phi\left( \sum_{i=1}^N w_ix_i \right)
$$

Ngo√†i ra c√≤n m·ªôt th·ª© m√† m√¨nh ch∆∞a nh·∫Øc ƒë·∫øn ƒë√≥ l√† **Bias**, t·ª©c l√† ta s·∫Ω c·ªông th√™m m·ªôt h·∫±ng s·ªë n·ªØa, g·ªçi l√† $b$ ƒëi.
$$
y = \phi\left( \sum_{i=1}^N w_ix_i + b \right)
$$

N·∫øu ƒë·∫∑t $b = w_0$ th√¨ ta s·∫Ω c√≥:
$$
y = \phi\left( w_0 + x_1w_1 + ... + w_Nx_N \right)
$$

V·∫≠y vector input $\mathbf{x}$ l√∫c n√†y tr·ªü th√†nh $\{1, x_1, ..., x_N\}$ (th√™m $1$ v√†o ƒë·∫ßu) v√† $\mathbf{w} = \{w_0, ..., w_N\}$. Nh∆∞ng m√¨nh ch·ªâ nh·∫Øc ƒë·∫øn th√¥i, t·ª´ ƒë√¢y v·ªÅ cu·ªëi lu√¥n m√¨nh s·∫Ω kh√¥ng d√πng ƒë·∫øn bias (b·ªüi v√¨ n√≥ l√†m m·ªçi th·ª© r·ªëi h∆°n v√† m√¨nh kh√¥ng ƒë·ªß kh·∫£ nƒÉng ƒë·ªÉ handle üò≠).

Ta g·ªçi h√†m $\phi$ l√† **h√†m activation**, ngo√†i ra ph·∫ßn t·ªï h·ª£p tuy·∫øn t√≠nh c√≥ khi ƒë∆∞·ª£c g·ªçi l√† *pre-activation* v√† k√≠ hi·ªáu l√† $a$ (nh∆∞ng ·ªü perceptron th√¨ ta ch∆∞a c·∫ßn k√≠ hi·ªáu v·∫≠y ƒë√¢u). Hi·ªán t·∫°i ƒë√£ c√≥ r·∫•t nhi·ªÅu h√†m activation, v√≠ d·ª• nh∆∞:
- H√†m sigmoid (m√¨nh s·∫Ω k√≠ hi·ªáu l√† $\sigma$):
$$
\phi(x) = \sigma(x) = \dfrac{1}{1 + e^x}
$$

<MdxPlotFunc
id="sigmoid_func"
fn="1/(1 + exp(-x))"
xrange={[-3, 3]}
yrange={[0, 1]}
/>

- H√†m tanh:
$$
\phi(x) = \tanh(x) = \dfrac{e^x - e^{-x}}{e^x + e^{-x}}
$$

<MdxPlotFunc
id="tanh_func"
fn="(exp(x) - exp(-x))/(exp(x) + exp(-x))"
xrange={[-3, 3]}
yrange={[-1, 1]}
/>

- H√†m ReLU [^6]:
$$
\phi(x) = \text{ReLU}(x) = \max(0, x) = \begin{cases}
x \hspace{10pt} \text{n·∫øu $x > 0$} \\
0 \hspace{10pt} \text{c√≤n l·∫°i}
\end{cases}
$$

<MdxPlotFunc
id="relu_func"
fn="max(0, x)"
xrange={[-3, 3]}
yrange={[-1, 1]}
/>

- Hay c√≥ th·ªÉ l√† h√†m Linear üíÄ:
$$
\phi(x) = x
$$

<MdxImage
src="https://drive.google.com/uc?export=view&id=1HPKVxzQAXHqGo0hqbvy04MvMklxHLzjN"
alt="Perceptron matrix"
caption="H√¨nh 3: Perceptron d∆∞·ªõi d·∫°ng ma tr·∫≠n v√† vector"
/>

<MdxImage
src="https://drive.google.com/uc?export=view&id=1HPp7pezze9EoEOEV8B9AWct3I7ZAeHtb"
alt="Perceptron matrix"
caption="H√¨nh 4: Perceptron d∆∞·ªõi d·∫°ng ma tr·∫≠n v√† vector (bi·ªÉu di·ªÖn kh√°c)"
/>

Nh√¨n h√¨nh 3 v√† 4 th√¨ ta c√≥ th·ªÉ ƒë∆∞a m·ªôt Perceptron v·ªÅ d·∫°ng ma tr·∫≠n tr·ªçng s·ªë $W$ v√† vector input $\mathbf{x}$, vector output $\mathbf{y}$ (c√°c vector ƒë·ªÅu l√† vector c·ªôt):
$$
\mathbf{y} = \phi(W \cdot \mathbf{x})
$$
trong ƒë√≥ $W$ s·∫Ω c√≥ s·ªë d√≤ng t∆∞∆°ng ·ª©ng v·ªõi chi·ªÅu c·ªßa $\mathbf{y}$ v√† s·ªë c·ªôt t∆∞∆°ng ·ª©ng v·ªõi s·ªë chi·ªÅu c·ªßa $\mathbf{x}$. N·∫øu m·ªü r·ªông h∆°n, s·ªë d√≤ng s·∫Ω t∆∞∆°ng ·ª©ng v·ªõi chi·ªÅu c·ªßa layer hi·ªán t·∫°i v√† s·ªë c·ªôt t∆∞∆°ng ·ª©ng v·ªõi layer tr∆∞·ªõc ƒë√≥.


### Deep Neural Network

<MdxImage
src="https://drive.google.com/uc?export=view&id=1HANtdbu1pmgET2kiZcutpucNnKantW-7"
alt="Multi-layer Perceptron Image"
caption="H√¨nh 5: M·ªôt v√≠ d·ª• v·ªÅ multi-layer perceptron (hay deep neural network)"
/>

C√≥ th·ªÉ th·∫•y, deep neural network ƒë∆∞·ª£c t·∫°o ra b·∫±ng c√°ch th√™m nhi·ªÅu l·ªõp hidden layer ·ªü gi·ªØa input layer v√† output layer. ƒê√¢y c≈©ng l√† l√Ω do m√† n√≥ c√≤n t√™n kh√°c l√† **Multi-layer Perceptron** (hay **MLP**) b·ªüi v√¨ ƒë∆°n gi·∫£n n√≥ l√† Perceptron v√† c√≥ th√™m nhi·ªÅu layer ·ªü gi·ªØa (Multi-layer).

<MdxImage
src="https://drive.google.com/uc?export=view&id=1HPIHTYCeoFAsMuEwXWweoGmkLM8OdAX3"
alt="Multi-layer Perceptron Image"
caption="H√¨nh 6: Deep Neural Network bi·ªÉu di·ªÖn d∆∞·ªõi d·∫°ng ma tr·∫≠n v√† vector"
/>

D·ª±a v√†o h√¨nh 6, ƒë·ªÉ t√≠nh ƒë∆∞·ª£c output c·ªßa m·ªôt Neural Network (g·ªçi t·∫Øt c·ªßa Deep Neural Network) ta s·∫Ω th·ª±c hi·ªán t√≠nh t·ª´ng hidden layer b·∫±ng c√°ch nh√¢n v·ªõi ma tr·∫≠n tr·ªçng s·ªë t∆∞∆°ng ·ª©ng. ƒê·ªÉ t√≠nh ƒë∆∞·ª£c output $\mathbf{y}$ ta ph·∫£i t√≠nh hidden layer $h_N$, t∆∞∆°ng t·ª± ƒë·ªÉ t√≠nh $h_{N}$ ph·∫£i t√≠nh ƒë∆∞·ª£c hidden layer $h_{N-1}$. V·∫≠y t·∫•t c·∫£ ch·ªâ l√† nh√¢n ma tr·∫≠n v·ªõi vector th√¥i hay sao üò≠. Th·∫≠t ra l√† c√≤n nhi·ªÅu h∆°n n·ªØa nh∆∞ng m√† ƒë∆°n gi·∫£n v·∫≠y l√† ƒë∆∞·ª£c r·ªìi.

N·∫øu ƒë·ªÉ √Ω, vi·ªác t√≠nh ƒë∆∞·ª£c output $\mathbf{y}$ th√¨ c·∫ßn b·∫Øt ƒë·∫ßu t·ª´ input $\mathbf{x}$ sau ƒë√≥ ti·∫øn ƒë·∫øn hidden layer th·ª© nh·∫•t, th·ª© hai, ... cho ƒë·∫øn hidden layer cu·ªëi c√πng, do ƒë√≥ n√≥ c√≤n ƒë∆∞·ª£c g·ªçi l√† b∆∞·ªõc **Forward** (ti·∫øn v·ªÅ ph√≠a tr∆∞·ªõc ·∫•y). Ta s·∫Ω c√≤n m·ªôt b∆∞·ªõc n·ªØa g·ªçi l√† **Backward** m√† m√¨nh v√† c√°c b·∫°n s·∫Ω c√πng t√¨m hi·ªÉu ·ªü ph·∫ßn 2 nh√©.

M·ªôt trong nh·ªØng d√≤ng code ƒë·∫ßu ti√™n khi code Neural Network ƒë√≥ ch√≠nh l√† ph·∫£i code ƒë∆∞·ª£c ma tr·∫≠n v√† t·∫•t c·∫£ ph√©p to√°n c·ªßa n√≥ v√≠ d·ª• nh∆∞ ph√©p nh√¢n, c·ªông, chuy·ªÉn v·ªã, ...

## Nh·ªØng d√≤ng code ƒë·∫ßu ti√™n: Ma tr·∫≠n

ƒê·∫ßu ti√™n ta ph·∫£i c√≥ 1 class ma tr·∫≠n. ·ªû ƒë√¢y m√¨nh ch·ªçn `std::vector` v√≠ n√≥ ƒë∆°n gi·∫£n, d·ªÖ s·ª≠ d·ª•ng c·ª±c k√¨. C√≤n t·∫°i sao l·∫°i d√πng `size_t` m√† kh√¥ng d√πng `int`, b·ªüi v√¨ n·∫øu b·∫°n g·ªçi `length()` c·ªßa vector th√¨ gi√° tr·ªã tr·∫£ v·ªÅ l√† `size_t` ch·ª© kh√¥ng ph·∫£i `int`, n√™n n·∫øu d√πng `int` t·ª©c l√† b·∫°n ƒëang √©p ki·ªÉu t·ª´ `size_t` v·ªÅ `int`.

> Ki·ªÉu `size_t` l√† m·ªôt ki·ªÉu d·ªØ li·ªáu s·ªë nguy√™n kh√¥ng d·∫•u v√† ƒëi·ªÅu ƒë·∫∑c bi·ªát l√† `size_t` kh√¥ng c√≥ k√≠ch th∆∞·ªõc c·ªë ƒë·ªãnh, trong khi `int` th√¨ s·∫Ω t·ª´ 2 cho ƒë·∫øn 4 bytes [^1].

Vi·ªác d√πng `const Matrix& mat` thay v√¨ `Matrix mat` s·∫Ω l√†m ƒë∆∞·ª£c 2 th·ª© khi c√°c b·∫°n truy·ªÅn v√†o tham s·ªë, th·ª© nh·∫•t l√† `&` s·∫Ω ch·ªâ ƒë·ªãnh compiler r·∫±ng ta s·∫Ω truy·ªÅn tham chi·∫øu (hay pass by reference) n·∫øu kh√¥ng c√≥ d·∫•u `&` th√¨ compiler s·∫Ω t·∫°o m·ªôt copy c·ªßa `mat` d·∫´n ƒë·∫øn kh√¥ng t·ªëi ∆∞u, ti·∫øp theo l√† `const`, b·ªüi v√¨ khi d√πng `&` ta c√≥ th·ªÉ thay ƒë·ªïi ƒë∆∞·ª£c gi√° tr·ªã c·ªßa `mat` trong h√†m n√™n d√πng `const` ƒë·ªÉ tr√°nh ƒëi·ªÅu n√†y x·∫£y ra.

```cpp title="matrix.h"
class Matrix 
{
public:
    size_t rows;
    size_t cols;
    std::vector<std::vector<double>> values;

    // T·∫°o m·ªôt ma tr·∫≠n v·ªõi 
    // s·ªë d√≤ng = rows
    // s·ªë c·ªôt = cols
    Matrix(size_t rows, size_t cols);

    // T·∫°o m·ªôt ma tr·∫≠n = ma tr·∫≠n mat (truy·ªÅn v√†o)
    Matrix(const Matrix& mat);

    // Fill t·∫•t c·∫£ ph·∫ßn t·ª≠ c·ªßa ma tr·∫≠n v·ªõi 1 gi√° tr·ªã nhi·ªÅu nh·∫•t
    // ƒêi·ªÅu n√†y gi√∫p d·ªÖ d√†ng t·∫°o ma tr·∫≠n 0
    // Fill(0)
    void Fill(double value);

    // In ma tr·∫≠n
    void Print();

    // Xem hai ma tr·∫≠n c√≥ b·∫±ng nhau kh√¥ng
    Matrix& operator=(const Matrix& mat);

    // C·ªông b·∫±ng ma tr·∫≠n
    Matrix& operator+=(const Matrix& mat);
    // Tr·ª´ b·∫±ng ma tr·∫≠n
    Matrix& operator-=(const Matrix& mat);
    // Chuy·ªÉn v·ªã ma tr·∫≠n
    Matrix T();
}

// C·ªông ma tr·∫≠n
inline Matrix operator+(Matrix mat1, const Matrix& mat2) 
{
    mat1 += mat2;
    return mat1;
}

// Tr·ª´ ma tr·∫≠n
inline Matrix operator-(Matrix mat1, const Matrix& mat2) 
{
    mat1 -= mat2;
    return mat1;
}

// Nh√¢n ma tr·∫≠n
inline Matrix operator*(const Matrix& mat1, const Matrix& mat2) 
{
    // Implement nh√¢n ma tr·∫≠n ·ªü ƒë√¢y ...
}
```

T·∫°i sao m√¨nh l·∫°i d√πng `inline` r·ªìi code th√™m operator `+=` chi cho ph·ª©c t·∫°p v·∫≠y, ch·ªâ c·∫ßn code `Matrix operator+(const Matrix& mat1, const Matrix& mat2)` l√† ƒë·ªß r·ªìi. Vi·ªác m√¨nh implement `+=` tr∆∞·ªõc sau ƒë√≥ d√πng l·∫°i `+=` ·ªü `+` th√¨ s·∫Ω ti·∫øt ki·ªám ƒë∆∞·ª£c vi·ªác ph·∫£i implement 2 c√°i, ngo√†i ra d√πng `inline` s·∫Ω t·ªëi ∆∞u h∆°n. Ti·∫øp theo vi·ªác truy·ªÅn `Matrix mat1` thay v√¨ `const Matrix& mat1` s·∫Ω copy lu√¥n `mat1` v√† d√πng `+=` s·∫Ω thay ƒë·ªïi `mat1` tr·ª±c ti·∫øp, cu·ªëi c√πng l√† return v·ªÅ copy c·ªßa `mat1` v√† ta xem n√≥ nh∆∞ l√† k·∫øt qu·∫£ c·ªßa `mat1 + mat2` [^2].

ƒê·ªÉ ƒë∆°n gi·∫£n ho√°, khi nh√¢n ma tr·∫≠n m√¨nh ch·ªâ c·∫ßn ch·∫°y t·ª´ng d√≤ng, ƒë·∫øn t·ª´ng c·ªôt, sau ƒë√≥ t√≠ch v√¥ h∆∞·ªõng l·∫°i, c√°i n√†y th√¨ d·ªÖ l√†m nh∆∞ng ƒë·ªô ph·ª©c t·∫°p s·∫Ω r∆°i v√†o $O(n^3)$, r·∫•t l√† l·ªõn ha, n·∫øu c√°c b·∫°n mu·ªën nhanh h∆°n th√¨ c√≥ th·ªÉ d√πng c√°c thu·∫≠t to√°n kh√°c (nh∆∞ng m√† nh∆∞ n√†y c≈©ng ƒë·ªß nhanh r·ªìi).

```cpp title="matrix.h"
inline Matrix operator*(const Matrix& mat1, const Matrix& mat2)
{
    assert(mat1.cols == mat2.rows);
    
    Matrix res(mat1.rows, mat2.cols);

    for (size_t i = 0; i < mat1.rows; i++)
    {
        for (size_t j = 0; j < mat2.cols; j++)
        {
            for (size_t k = 0; k < mat1.cols; k++)
            {
                res.values[i][j] += mat1.values[i][k]*mat2.values[k][j];
            }
        }
    }

    return res;
}
```

Cu·ªëi c√πng s·∫Ω l√† implement c√°c h√†m c√≤n l·∫°i c·ªßa class h√†m ma tr·∫≠n. L∆∞u √Ω khi kh·ªüi t·∫°o vector l·ªìng vector nh·ªõ kh·ªüi t·∫°o vector ph√≠a b√™n trong [^3], n·∫øu kh√¥ng n√≥ s·∫Ω d·ªÖ b·ªã bug ƒë·∫•y.

> C√°i m√† `:rows(rows),cols(cols)` ·∫•y ƒë∆∞·ª£c g·ªçi l√† initializer list [^4]. ƒê√¢y l√† m·ªôt c√°ch ƒë·ªÉ kh·ªüi t·∫°o bi·∫øn cho class trong C++.

```cpp title="matrix.cpp"
Matrix::Matrix(size_t rows, size_t cols)
: rows(rows)
, cols(cols)
, values(rows, std::vector<double>(cols))
{}

Matrix::Matrix(const Matrix& mat)
: rows(mat.rows)
, cols(mat.cols)
, values(rows, std::vector<double>(cols))
{
    for (size_t i = 0; i < rows; i++)
    {
        for (size_t j = 0; j < cols; j++)
        {
            values[i][j] = mat.values[i][j];
        }
    }
}
```

ƒê·ªëi v·ªõi ma tr·∫≠n chuy·ªÉn v·ªã, th√¨ ta c≈©ng d√πng thu·∫≠t to√°n ƒë∆°n gi·∫£n theo c√¥ng th·ª©c sau:
$$
(A^T)[j][i] = A[i][j]
$$

```cpp title="matrix.cpp"
Matrix Matrix::T()
{
    Matrix mat(cols, rows);
    for (size_t i = 0; i < rows; i++)
    {
        for (size_t j = 0; j < cols; j++)
        {
            mat.values[j][i] = this->values[j][i];
        }
    }
    return mat;
}
```

Ngo√†i ra c√≤n operator `+=, -=, =` m√† ch√∫ng ta c≈©ng ph·∫£i implement, m√¨nh nghƒ© code n√≥ kh√° d·ªÖ r·ªìi n√™n s·∫Ω kh√¥ng gi·∫£i th√≠ch nhi·ªÅu. Tr∆∞·ªõc khi c·ªông, tr·ª´ hay so s√°nh hai ma tr·∫≠n, ta ph·∫£i so s√°nh hai ma tr·∫≠n xem c√≥ c√πng chi·ªÅu v·ªõi nhau kh√¥ng th√¥ng qua m·ªôt h√†m ph·ª• l√† `CheckDimension`.

```cpp title="matrix.cpp"
void Matrix::CheckDimension(const Matrix& mat1, const Matrix& mat2)
{
    assert(mat1.rows == mat2.rows && mat2.cols == mat2.cols);
}

Matrix& Matrix::operator=(const Matrix& mat)
{
    this->cols = mat.cols;
    this->rows = mat.rows;
    for (size_t i = 0; i < rows; i++)
    {
        for (size_t j = 0; j < cols; j++)
        {
            values[i][j] = mat.values[i][j];
        }
    }
    return *this;
}

Matrix& Matrix::operator-=(const Matrix& mat)
{
    CheckDimension((*this), mat);
    for (size_t i = 0; i < rows; i++)
    {
        for (size_t j = 0; j < cols; j++)
        {
            this->values[i][j] -= mat.values[i][j];
        }
    }
    return *this;
}

Matrix& Matrix::operator+=(const Matrix& mat)
{
    CheckDimension((*this), mat);
    for (size_t i = 0; i < rows; i++)
    {
        for (size_t j = 0; j < cols; j++)
        {
            this->values[i][j] += mat.values[i][j];
        }
    }
    return *this;
}
```

Nh∆∞ v·∫≠y l√† g·∫ßn xong class ma tr·∫≠n r·ªìi, th·∫ø nh∆∞ng ta v·∫´n c·∫ßn m·ªôt s·ªë h√†m ph·ª• tr·ª£ n·ªØa. ƒê·∫ßu ti√™n ch√≠nh l√† `Flatten`, t·ª©c l√† ta s·∫Ω ƒë∆∞a ma tr·∫≠n 2 chi·ªÅu v·ªÅ th√†nh vector 1 chi·ªÅu, c√°i n√†y c·ª±c quan tr·ªçng n·∫øu d·ªØ li·ªáu l√† ·∫£nh, ·∫£nh th√¥ng th∆∞·ªùng s·∫Ω ·ªü d·∫°ng 2 chi·ªÅu, nh∆∞ng m√† input c·ªßa Neural Network l·∫°i l√† 1 chi·ªÅu. M√¨nh implement h√†m `Flatten` nh∆∞ sau: 

```cpp title="matrix.h"
class Matrix
{
    // ...
    static Matrix Flatten(const Matrix& mat);
    // ...
}
```

Vi·ªác m√¨nh ch·ªçn `static` l√† b·ªüi v√¨ m√¨nh kh√¥ng mu·ªën h√†m `Flatten` ph·∫£i c√≥ object m·ªõi d√πng ƒë∆∞·ª£c, ki·ªÉu nh∆∞ ta c√≥ m·ªôt object `Matrix mat` b·∫•t k√¨ th√¨ c·ª© g·ªçi `Matrix::Flatten(mat)` l√† ƒë∆∞·ª£c.

```cpp title="matrix.cpp"
Matrix Matrix::Flatten(const Matrix& mat)
{
    // ·ªû ƒë√¢y vector n chi·ªÅu ƒë∆∞·ª£c xem nh∆∞ l√† m·ªôt ma tr·∫≠n 
    // v·ªõi 1 c·ªôt v√† n d√≤ng (vector c·ªôt)
    Matrix res(mat.cols * mat.rows, 1);
    for (size_t i = 0; i < mat.rows; i++)
    {
        for (size_t j = 0; j < mat.cols; j++)
        {
            res.values[i*mat.rows + j][0] = mat.values[i][j];
        }
    }
    return res;
}
```

Ti·∫øp theo l√† m√¨nh ph·∫£i t√¨m c√°ch ƒë·ªÉ kh·ªüi t·∫°o weight cho Neural Network m·ªôt c√°ch t·ªëi ∆∞u nh·∫•t, theo [^5] th√¨ kh·ªüi t·∫°o tr·ªçng s·ªë nh∆∞ sau: V·ªõi m·ªói tr·ªçng s·ªë $w$, ta l·∫•y $w$ t·ª´ m·ªôt ph√¢n ph·ªëi chu·∫©n c√≥ trung b√¨nh l√† $0$ v√† ph∆∞∆°ng sai l√† $\sqrt{\left( 2/n \right)}$ v·ªõi $n$ l√† s·ªë chi·ªÅu c·ªßa vector input (hay ƒë√∫ng h∆°n l√† chi·ªÅu c·ªßa layer ph√≠a tr∆∞·ªõc).

```cpp title="matrix.h"
class Matrix
{
    // ...
    static Matrix Randomized(size_t rows, size_t cols);
    // ...
}
```

```cpp title="matrix.cpp"
#include <random>

// ...

Matrix Matrix::Randomized(size_t rows, size_t cols)
{
    // generate random in uniform distribution
    std::random_device rand_dev;
    std::mt19937 generator(rand_dev());

    // https://cs231n.github.io/neural-networks-2/#init
    double var = 2.0f / sqrt(static_cast<double>(cols)); 
    std::uniform_real_distribution<double> distr(0, var);

    Matrix res(rows, cols);
    for (size_t i = 0; i < rows; i++)
    {
        for (size_t j = 0; j < cols; j++)
        {
            res.values[i][j] = distr(generator);
        }
    }

    return res;
}
```

√Ä c√≤n c√°i cu·ªëi c√πng, m√¨nh c·∫ßn d√πng ƒë∆∞·ª£c m·ªôt h√†m Element-wise tr√™n class Matrix (v√≠ d·ª• nh∆∞ d√πng Activation l√™n class Matrix). M√¨nh s·∫Ω vi·∫øt h√†m `Apply()` (gi·ªëng t∆∞∆°ng t·ª± nh∆∞ h√†m `apply` trong `pandas`), ```std::function<double(double)>``` nghƒ©a l√† m·ªôt h√†m c√≥ m·ªôt tham s·ªë ki·ªÉu double (c√°i ngo·∫∑c tr√≤n double √Ω) v√† ki·ªÉu tr·∫£ v·ªÅ l√† double (ph√≠a ngo√†i), t·ª©c l√† ƒë√¢y l√† ki·ªÉu d·ªØ li·ªáu c·ªßa h√†m (m√¨nh truy·ªÅn h√†m nh∆∞ tham s·ªë) c·ªßa C++11 tr·ªü l√™n.

```cpp title="matrix.h"
class Matrix
{
    // ...
    // Apply a function to matrix
    Matrix Apply(const std::function<double(double)>& func);
    // ...
}
```

```cpp title="matrix.cpp"
Matrix Matrix::Apply(const std::function<double(double)>& func)
{
    Matrix mat(rows, cols);
    for (size_t i = 0; i < rows; i++)
    {
        for (size_t j = 0; j < cols; j++)
        {
            mat.values[i][j] = func(this->values[i][j]);
        }
    }
    return mat;
}
```

V·∫≠y l√† xong class Ma tr·∫≠n r·ªìi ·∫•y, c·∫£m ∆°n c√°c b·∫°n ƒë√£ ƒë·ªçc ƒë·∫øn ƒë√¢y hehe üçâ. ·ªû ph·∫ßn sau [Neural Network ƒë∆°n gi·∫£n v·ªõi C++ (Ph·∫ßn 2)](/blog/cpp-nn-2) ta s·∫Ω ƒëi qua k·ªπ thu·∫≠t Backpropagation (hay c√≤n g·ªçi l√† Backward) v√† c√°ch train Deep Neural Network.

<MdxBib
source="cpp-nn.bib"
/>

[^1]: https://stackoverflow.com/questions/131803/unsigned-int-vs-size-t
[^2]: https://stackoverflow.com/questions/4421706/what-are-the-basic-rules-and-idioms-for-operator-overloading
[^3]: https://stackoverflow.com/questions/13121469/initializing-a-vector-of-vectors-having-a-fixed-size-with-boost-assign
[^4]: https://en.cppreference.com/w/cpp/language/constructor
[^5]: https://cs231n.github.io/neural-networks-2/#init
[^6]: https://en.wikipedia.org/wiki/Rectifier_(neural_networks)